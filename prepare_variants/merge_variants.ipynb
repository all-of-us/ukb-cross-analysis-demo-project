{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we merge the AoU and UKB variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hail as hl\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The AoU matrix table for the alpha2 release was created via notebook 'create_matrix_tables.ipynb'. It contains all samples and variants for the alpha2 release.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOU_MT = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/aou/alpha2/cohort.mt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The UKB matrix table was created via notebook 'create_matrix_tables' and then repartitioned via notebook 'redo_partitions'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKB_MT = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/ukb/exomes/full_dataset_fewer_partitions.mt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the two matrix tables with the same NUM_MT_READ_PARTITIONS partitions from the UKB matrix table.\n",
    "NUM_MT_READ_PARTITIONS = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXOME_REGIONS = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/ukb/exomes/xgen_plus_spikein.GRCh38.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVALS_TO_EXAMINE = [f'chr{chrom}' for chrom in range(6, 21)]\n",
    "INTERVALS_TO_EXAMINE_NAME = '_'.join(INTERVALS_TO_EXAMINE).replace(':', 'range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_BUCKET = os.getenv(\"WORKSPACE_BUCKET\")\n",
    "DATESTAMP = time.strftime('%Y%m%d')\n",
    "TIMESTAMP = time.strftime('%Y%m%d_%H%M%S')\n",
    "WORK_DIR = !pwd\n",
    "\n",
    "# Outputs\n",
    "AOU_ROWS_TAB = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/merged/{DATESTAMP}/aou_rows-{INTERVALS_TO_EXAMINE_NAME}.tab'\n",
    "UKB_ROWS_TAB = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/merged/{DATESTAMP}/ukb_rows-{INTERVALS_TO_EXAMINE_NAME}.tab'\n",
    "MERGED_MT = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/merged/{DATESTAMP}/merged-filtered-{INTERVALS_TO_EXAMINE_NAME}.mt'\n",
    "AOU_ONLY_TAB = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/merged/{DATESTAMP}/aou_only-filtered-{INTERVALS_TO_EXAMINE_NAME}.tab'\n",
    "UKB_ONLY_TAB = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/merged/{DATESTAMP}/ukb_only-filtered-{INTERVALS_TO_EXAMINE_NAME}.tab'\n",
    "HAIL_LOG = f'{WORK_DIR[0]}/hail-merge-variants-{TIMESTAMP}.log'\n",
    "HAIL_LOG_DIR_FOR_PROVENANCE = f'{os.getenv(\"WORKSPACE_BUCKET\")}/hail-logs/{DATESTAMP}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MERGED_MT)\n",
    "print(AOU_ONLY_TAB)\n",
    "print(UKB_ONLY_TAB)\n",
    "print(HAIL_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {AOU_MT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {UKB_MT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also https://towardsdatascience.com/fetch-failed-exception-in-apache-spark-decrypting-the-most-common-causes-b8dff21075c\n",
    "# See https://spark.apache.org/docs/2.4.7/configuration.html\n",
    "\n",
    "EXTRA_SPARK_CONFIG = {\n",
    "    # If set to \"true\", performs speculative execution of tasks. This means if one or more tasks are running\n",
    "    # slowly in a stage, they will be re-launched.\n",
    "    'spark.speculation': 'true', # Default is false.\n",
    "    \n",
    "    # Fraction of tasks which must be complete before speculation is enabled for a particular stage.\n",
    "    'spark.speculation.quantile': '0.95', # Default is 0.75\n",
    "\n",
    "    # Default timeout for all network interactions. This config will be used in place of \n",
    "    # spark.core.connection.ack.wait.timeout, spark.storage.blockManagerSlaveTimeoutMs, \n",
    "    # spark.shuffle.io.connectionTimeout, spark.rpc.askTimeout or spark.rpc.lookupTimeout if they are not configured.\n",
    "    'spark.network.timeout': '180s', # Default is 120s\n",
    "        \n",
    "    # (Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is set to a\n",
    "    # non-zero value. This retry logic helps stabilize large shuffles in the face of long GC pauses or transient\n",
    "    # network connectivity issues.\n",
    "    'spark.shuffle.io.maxRetries': '10',  # Default is 3\n",
    "    \n",
    "    # (Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying is 15 seconds\n",
    "    # by default, calculated as maxRetries * retryWait.\n",
    "    'spark.shuffle.io.retryWait': '15s',  # Default is 5s\n",
    "    \n",
    "    # Number of failures of any particular task before giving up on the job. The total number of failures spread\n",
    "    # across different tasks will not cause the job to fail; a particular task has to fail this number of attempts.\n",
    "    # Should be greater than or equal to 1. Number of allowed retries = this value - 1.\n",
    "    'spark.task.maxFailures': '10', # Default is 4.\n",
    "\n",
    "    # Number of consecutive stage attempts allowed before a stage is aborted.\n",
    "    'spark.stage.maxConsecutiveAttempts': '10' # Default is 4.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.init(spark_conf=EXTRA_SPARK_CONFIG,\n",
    "        min_block_size=50,\n",
    "        default_reference='GRCh38',\n",
    "        log=HAIL_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = hl.spark_context()\n",
    "config = sc._conf.getAll()\n",
    "config.sort()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load exome capture regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions = hl.import_bed(EXOME_REGIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions.aggregate(hl.agg.counter(ukb_exome_capture_regions.interval.start.contig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read UKB exomes matrix table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exomes = hl.read_matrix_table(UKB_MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exomes.n_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.hail.is/t/improving-pipeline-performance/1344\n",
    "new_partitions = ukb_exomes._calculate_new_partitions(NUM_MT_READ_PARTITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exomes = hl.read_matrix_table(UKB_MT, _intervals=new_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exomes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to include only our genomic intervals of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exomes = hl.filter_intervals(\n",
    "    ukb_exomes,\n",
    "    [hl.parse_locus_interval(x) for x in INTERVALS_TO_EXAMINE],\n",
    "    keep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to include only exonic variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the shuffle happen all at once by materializing the relevant row keys to a file.\n",
    "relevant_ukb_rows = ukb_exomes.filter_rows(\n",
    "    hl.is_defined(ukb_exome_capture_regions[ukb_exomes.locus])).rows().select().checkpoint(UKB_ROWS_TAB)\n",
    "ukb_exomes = ukb_exomes.semi_join_rows(relevant_ukb_rows) # alias for filter_rows(hl.is_defined ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read AoU matrix table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_wgs = hl.read_matrix_table(AOU_MT, _intervals=new_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_wgs.n_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_wgs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to include only our genomic intervals of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_wgs = hl.filter_intervals(\n",
    "    aou_wgs,\n",
    "    [hl.parse_locus_interval(x) for x in INTERVALS_TO_EXAMINE],\n",
    "    keep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to include only exonic variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the shuffle happen all at once by materializing the relevant row keys to a file.\n",
    "relevant_aou_rows = aou_wgs.filter_rows(\n",
    "    hl.is_defined(ukb_exome_capture_regions[aou_wgs.locus])).rows().select().checkpoint(AOU_ROWS_TAB)\n",
    "aou_wgs = aou_wgs.semi_join_rows(relevant_aou_rows) # alias for filter_rows(hl.is_defined ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omit samples that fail QC thresholds\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omit variants that fail QC thresholds\n",
    "\n",
    "TODO: more here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omit variants with filter flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_wgs = aou_wgs.filter_rows(hl.len(aou_wgs.filters) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exomes = ukb_exomes.filter_rows(hl.is_missing(ukb_exomes.filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_all = datetime.now()\n",
    "print(start_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the multi-allelic sites\n",
    "\n",
    "See also https://hail.is/docs/0.2/methods/genetics.html#hail.methods.split_multi_hts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, do not pass the biallelic variants to the split method,\n",
    "# just add the corresponding annotations.\n",
    "aou_bi = aou_wgs.filter_rows(hl.len(aou_wgs.alleles) == 2)\n",
    "aou_bi = aou_bi.annotate_rows(a_index = 1)\n",
    "aou_bi = aou_bi.annotate_rows(was_split = False)\n",
    "\n",
    "# Split the multi-allelic sites into biallelic sites.\n",
    "aou_multi = aou_wgs.filter_rows(hl.len(aou_wgs.alleles) > 2)\n",
    "aou_split = hl.split_multi_hts(aou_multi,\n",
    "                               keep_star=False,\n",
    "                               left_aligned=False,\n",
    "                               vep_root='vep',\n",
    "                               permit_shuffle=False)\n",
    "\n",
    "# Union the two collections and include only the row and entry fields that are needed.\n",
    "aou_prepared = aou_split.union_rows(aou_bi)\n",
    "aou_prepared = aou_prepared.annotate_cols(cohort='AOU')\n",
    "aou_prepared = aou_prepared.key_cols_by(aou_prepared.s, aou_prepared.cohort)\n",
    "aou_prepared = aou_prepared.select_entries(aou_prepared.GT)\n",
    "aou_prepared = aou_prepared.select_rows(aou_qual=aou_prepared.qual,\n",
    "                                        aou_filters=aou_prepared.filters,\n",
    "                                        aou_info=aou_prepared.info,\n",
    "                                        aou_a_index = aou_prepared.a_index,\n",
    "                                        aou_was_split=aou_prepared.was_split,\n",
    "                                       )\n",
    "\n",
    "aou_prepared.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, do not pass the biallelic variants to the split method,\n",
    "# just add the corresponding annotations.\n",
    "ukb_bi = ukb_exomes.filter_rows(hl.len(ukb_exomes.alleles) == 2)\n",
    "ukb_bi = ukb_bi.annotate_rows(a_index = 1)\n",
    "ukb_bi = ukb_bi.annotate_rows(was_split = False)\n",
    "\n",
    "# Split the multi-allelic sites into biallelic sites.\n",
    "ukb_multi = ukb_exomes.filter_rows(hl.len(ukb_exomes.alleles) > 2)\n",
    "ukb_split = hl.split_multi_hts(ukb_multi,\n",
    "                               keep_star=False,\n",
    "                               left_aligned=False,\n",
    "                               vep_root='vep',\n",
    "                               permit_shuffle=False)\n",
    "\n",
    "# Union the two collections and include only the row and entry fields that are needed.\n",
    "ukb_prepared = ukb_split.union_rows(ukb_bi)\n",
    "ukb_prepared = ukb_prepared.annotate_cols(cohort='UKB')\n",
    "ukb_prepared = ukb_prepared.key_cols_by(ukb_prepared.s, ukb_prepared.cohort)\n",
    "ukb_prepared = ukb_prepared.select_entries(ukb_prepared.GT)\n",
    "ukb_prepared = ukb_prepared.select_rows(ukb_qual=ukb_prepared.qual,\n",
    "                                        ukb_filters=ukb_prepared.filters,\n",
    "                                        ukb_info=ukb_prepared.info,\n",
    "                                        ukb_a_index = ukb_prepared.a_index,\n",
    "                                        ukb_was_split=ukb_prepared.was_split,\n",
    "                                       )\n",
    "\n",
    "ukb_prepared.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** this will retain the row fields from AoU but not those from UKB. We could also add an annotation step to add those row UKB row fields to the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = aou_prepared.union_cols(ukb_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection.write(MERGED_MT, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AoU - UKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_only = aou_prepared.rows().anti_join(ukb_prepared.rows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_only.write(AOU_ONLY_TAB, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute UKB - AoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_only = ukb_prepared.rows().anti_join(aou_prepared.rows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_only.write(UKB_ONLY_TAB, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_all = datetime.now()\n",
    "print(end_all)\n",
    "print(end_all - start_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the Hail log to the workspace bucket so that we can retain it.\n",
    "!gzip --keep {HAIL_LOG}\n",
    "!gsutil cp {HAIL_LOG}.gz {HAIL_LOG_DIR_FOR_PROVENANCE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
