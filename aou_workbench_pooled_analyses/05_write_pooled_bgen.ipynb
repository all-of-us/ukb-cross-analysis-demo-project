{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write merged AoU + UKB BGEN file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use [HAIL](https://hail.is/) to write the matrix table to a BGEN file for use with other tools such as [PLINK2](https://www.cog-genomics.org/plink/2.0/) and [regenie](https://rgcgithub.github.io/regenie/).\n",
    "\n",
    "Note that this work is part of a larger project to [Demonstrate the Potential for Pooled Analysis of All of Us and UK Biobank Genomic Data](https://docs.google.com/document/d/19ZS0z_-7FEM37pNDAXaWaqBSLnqyd9MZEkiOmtF3n_0/edit#). Specifically this is for the portion of the project that is the **pooled** analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Cloud Environment</b>: This notebook was written for use on the <i>All of Us</i> Workbench.\n",
    "    <ul>\n",
    "        <li>Use \"Recommended Environment\" <kbd><b>Hail Genomics Analysis</b></kbd> which creates compute type <kbd>Dataproc Cluster</kbd> with reasonable defaults for CPU, RAM, disk, and number of workers. If you like, you can increase the number of workers to make this job complete faster.</li>\n",
    "        <li>This notebook can take several hours to run. Recommend that it is run in the background via <kbd>run_notebook_in_the_background</kbd>.</li>\n",
    "        <ul>\n",
    "            <li>chr1 - chr22 <b>TODO(deflaux) add these details</b></li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hail as hl\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters. See https://papermill.readthedocs.io/en/latest/usage-parameterize.html\n",
    "\n",
    "#---[ Inputs ]---\n",
    "# This matrix table was created via notebook `aou_workbench_pooled_analyses/03_merge_variants.ipynb`. \n",
    "MERGED_MT = [\n",
    "    'gs://fc-secure-e53e4a44-7fe2-42b7-89b7-01aae1e399f7/data/pooled/geno/20220209/merged-aou3-ukb-filtered-chr1-chr22.mt'\n",
    "]\n",
    "INTERVALS_TO_EXAMINE = ['chr1-chr22']\n",
    "INTERVALS_TO_EXAMINE_NAME = '_'.join(INTERVALS_TO_EXAMINE).replace(':', 'range')\n",
    "\n",
    "#---[ Outputs ]---\n",
    "# Create a timestamp for a folder of results generated today.\n",
    "DATESTAMP = time.strftime('%Y%m%d')\n",
    "TIMESTAMP = time.strftime('%Y%m%d_%H%M%S')\n",
    "WORK_DIR = !pwd\n",
    "\n",
    "OUTPUT_BGEN = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/pooled/geno/{DATESTAMP}/aou-alpha3-ukb-{INTERVALS_TO_EXAMINE_NAME}' # Hail will add the .bgen suffix.\n",
    "HAIL_LOG = f'{WORK_DIR[0]}/hail-write-bgen-{TIMESTAMP}.log'\n",
    "HAIL_LOG_DIR_FOR_PROVENANCE = f'{os.getenv(\"WORKSPACE_BUCKET\")}/hail-logs/{DATESTAMP}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mt in MERGED_MT:\n",
    "    !gsutil ls {mt}\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also https://towardsdatascience.com/fetch-failed-exception-in-apache-spark-decrypting-the-most-common-causes-b8dff21075c\n",
    "# See https://spark.apache.org/docs/2.4.7/configuration.html\n",
    "\n",
    "EXTRA_SPARK_CONFIG = {\n",
    "    # If set to \"true\", performs speculative execution of tasks. This means if one or more tasks are running\n",
    "    # slowly in a stage, they will be re-launched.\n",
    "    'spark.speculation': 'true', # Default is false.\n",
    "    \n",
    "    # Fraction of tasks which must be complete before speculation is enabled for a particular stage.\n",
    "    'spark.speculation.quantile': '0.95', # Default is 0.75\n",
    "\n",
    "    # Default timeout for all network interactions. This config will be used in place of \n",
    "    # spark.core.connection.ack.wait.timeout, spark.storage.blockManagerSlaveTimeoutMs, \n",
    "    # spark.shuffle.io.connectionTimeout, spark.rpc.askTimeout or spark.rpc.lookupTimeout if they are not configured.\n",
    "    'spark.network.timeout': '180s', # Default is 120s\n",
    "        \n",
    "    # (Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is set to a\n",
    "    # non-zero value. This retry logic helps stabilize large shuffles in the face of long GC pauses or transient\n",
    "    # network connectivity issues.\n",
    "    'spark.shuffle.io.maxRetries': '10',  # Default is 3\n",
    "    \n",
    "    # (Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying is 15 seconds\n",
    "    # by default, calculated as maxRetries * retryWait.\n",
    "    'spark.shuffle.io.retryWait': '15s',  # Default is 5s\n",
    "    \n",
    "    # Number of failures of any particular task before giving up on the job. The total number of failures spread\n",
    "    # across different tasks will not cause the job to fail; a particular task has to fail this number of attempts.\n",
    "    # Should be greater than or equal to 1. Number of allowed retries = this value - 1.\n",
    "    'spark.task.maxFailures': '10', # Default is 4.\n",
    "\n",
    "    # Number of consecutive stage attempts allowed before a stage is aborted.\n",
    "    'spark.stage.maxConsecutiveAttempts': '10' # Default is 4.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.init(spark_conf=EXTRA_SPARK_CONFIG,\n",
    "        min_block_size=50,\n",
    "        default_reference='GRCh38',\n",
    "        log=HAIL_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = hl.spark_context()\n",
    "config = sc._conf.getAll()\n",
    "config.sort()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the matrix table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(MERGED_MT)):\n",
    "    mt = hl.read_matrix_table(MERGED_MT[i])\n",
    "    print(f'{mt.n_partitions()} {MERGED_MT[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = hl.read_matrix_table(MERGED_MT[0])\n",
    "\n",
    "for i in range(1, len(MERGED_MT)):\n",
    "    merged = merged.union_rows(hl.read_matrix_table(MERGED_MT[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to our intervals of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(INTERVALS_TO_EXAMINE) > 0:\n",
    "    merged = hl.filter_intervals(\n",
    "        merged,\n",
    "        [hl.parse_locus_interval(x) for x in INTERVALS_TO_EXAMINE],\n",
    "        keep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a single value to hold both parts of the column key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.annotate_cols(cohort_key = merged.s + '_' + merged.cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.key_cols_by(merged.cohort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an rsid\n",
    "\n",
    "This is needed by plink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.annotate_rows(\n",
    "    rsid = merged.locus.contig + '_' + hl.str(merged.locus.position)\n",
    "            + '_' + merged.alleles[0] + '_' + merged.alleles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the matrix table to BGEN\n",
    "\n",
    "https://hail.is/docs/0.2/methods/impex.html#hail.methods.export_bgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homref_gp = hl.literal([1.0, 0.0, 0.0])\n",
    "het_gp = hl.literal([0.0, 1.0, 0.0])\n",
    "homvar_gp = hl.literal([0.0, 0.0, 1.0])\n",
    "\n",
    "merged = merged.annotate_entries(\n",
    "    GP = hl.case()\n",
    "        .when(merged.GT.is_hom_ref(), homref_gp)\n",
    "        .when(merged.GT.is_het(), het_gp)\n",
    "        .default(homvar_gp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.methods.export_bgen(mt=merged, output=OUTPUT_BGEN, gp=merged.GP, rsid=merged.rsid, parallel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.methods.index_bgen(OUTPUT_BGEN + '.bgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the Hail log to the workspace bucket so that we can retain it.\n",
    "!gzip --keep {HAIL_LOG}\n",
    "!gsutil cp {HAIL_LOG}.gz {HAIL_LOG_DIR_FOR_PROVENANCE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
