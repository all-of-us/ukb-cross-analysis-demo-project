{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write filtered AoU BGEN file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we write data from the *All of Us* whole genome sequence matrix table to a BGEN file for use with other tools such as [PLINK2](https://www.cog-genomics.org/plink/2.0/) and [regenie](https://rgcgithub.github.io/regenie/).\n",
    "\n",
    "Note that this work is part of a larger project to [Demonstrate the Potential for Pooled Analysis of All of Us and UK Biobank Genomic Data](https://docs.google.com/document/d/19ZS0z_-7FEM37pNDAXaWaqBSLnqyd9MZEkiOmtF3n_0/edit#). Specifically this is for the portion of the project that is the **siloed** analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Cloud Environment</b>: This notebook was written for use on the <i>All of Us</i> Workbench.\n",
    "    <ul>\n",
    "        <li>Use \"Recommended Environment\" <kbd><b>Hail Genomics Analysis</b></kbd> which creates compute type <kbd>Dataproc Cluster</kbd> with reasonable defaults for CPU, RAM, disk, and number of workers. If you like, you can increase the number of workers to make this job complete faster.</li>\n",
    "        <li>This notebook can take several hours to run. Recommend that it is run in the background via <kbd>run_notebook_in_the_background</kbd>.</li>\n",
    "        <ul>\n",
    "            <li>chr21 took about 5 hours with 5 worker nodes.</li>\n",
    "            <li>chr1 - chr22 <b>TODO(margaret) besure to write down how long this takes with how many worker nodes</b></li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hail as hl\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the exome capture regions\n",
    "\n",
    "For details, see https://biobank.ndph.ox.ac.uk/ukb/refer.cgi?id=3803."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget -nd -nv biobank.ndph.ox.ac.uk/ukb/ukb/auxdata/xgen_plus_spikein.GRCh38.bed\n",
    "\n",
    "gsutil cp xgen_plus_spikein.GRCh38.bed ${WORKSPACE_BUCKET}/data/ukb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters. See https://papermill.readthedocs.io/en/latest/usage-parameterize.html\n",
    "\n",
    "#---[ Inputs ]---\n",
    "# Matrix table was provided by AoU.\n",
    "AOU_MT = 'gs://fc-secure-4adb21f6-46f4-4a79-99f9-afd63890c6d0/data/beta/beta_wgs_98622.mt'\n",
    "# This file is from https://biobank.ndph.ox.ac.uk/ukb/refer.cgi?id=3803.\n",
    "EXOME_REGIONS = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/ukb/xgen_plus_spikein.GRCh38.bed'\n",
    "# Use autosomes only.\n",
    "INTERVALS_TO_EXAMINE = ['chr1-chr22']\n",
    "INTERVALS_TO_EXAMINE = ['chr21']  # <----------- REMOVE THIS LATER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "INTERVALS_TO_EXAMINE_NAME = '_'.join(INTERVALS_TO_EXAMINE).replace(':', 'range')\n",
    "\n",
    "#---[ Outputs ]---\n",
    "# Create a timestamp for a folder of results generated today.\n",
    "DATESTAMP = time.strftime('%Y%m%d')\n",
    "TIMESTAMP = time.strftime('%Y%m%d_%H%M%S')\n",
    "WORK_DIR = !pwd\n",
    "\n",
    "OUTPUT_BGEN = f'{os.getenv(\"WORKSPACE_BUCKET\")}/data/aou/geno/{DATESTAMP}/aou-alpha3-{INTERVALS_TO_EXAMINE_NAME}' # Hail will add the .bgen suffix.\n",
    "HAIL_LOG = f'{WORK_DIR[0]}/hail-write-filtered-bgen-{TIMESTAMP}.log'\n",
    "HAIL_LOG_DIR_FOR_PROVENANCE = f'{os.getenv(\"WORKSPACE_BUCKET\")}/hail-logs/{DATESTAMP}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {AOU_MT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also https://towardsdatascience.com/fetch-failed-exception-in-apache-spark-decrypting-the-most-common-causes-b8dff21075c\n",
    "# See https://spark.apache.org/docs/2.4.7/configuration.html\n",
    "\n",
    "EXTRA_SPARK_CONFIG = {\n",
    "    # If set to \"true\", performs speculative execution of tasks. This means if one or more tasks are running\n",
    "    # slowly in a stage, they will be re-launched.\n",
    "    'spark.speculation': 'true', # Default is false.\n",
    "    \n",
    "    # Fraction of tasks which must be complete before speculation is enabled for a particular stage.\n",
    "    'spark.speculation.quantile': '0.95', # Default is 0.75\n",
    "\n",
    "    # Default timeout for all network interactions. This config will be used in place of \n",
    "    # spark.core.connection.ack.wait.timeout, spark.storage.blockManagerSlaveTimeoutMs, \n",
    "    # spark.shuffle.io.connectionTimeout, spark.rpc.askTimeout or spark.rpc.lookupTimeout if they are not configured.\n",
    "    'spark.network.timeout': '180s', # Default is 120s\n",
    "        \n",
    "    # (Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is set to a\n",
    "    # non-zero value. This retry logic helps stabilize large shuffles in the face of long GC pauses or transient\n",
    "    # network connectivity issues.\n",
    "    'spark.shuffle.io.maxRetries': '10',  # Default is 3\n",
    "    \n",
    "    # (Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying is 15 seconds\n",
    "    # by default, calculated as maxRetries * retryWait.\n",
    "    'spark.shuffle.io.retryWait': '15s',  # Default is 5s\n",
    "    \n",
    "    # Number of failures of any particular task before giving up on the job. The total number of failures spread\n",
    "    # across different tasks will not cause the job to fail; a particular task has to fail this number of attempts.\n",
    "    # Should be greater than or equal to 1. Number of allowed retries = this value - 1.\n",
    "    'spark.task.maxFailures': '10', # Default is 4.\n",
    "\n",
    "    # Number of consecutive stage attempts allowed before a stage is aborted.\n",
    "    'spark.stage.maxConsecutiveAttempts': '10' # Default is 4.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.init(spark_conf=EXTRA_SPARK_CONFIG,\n",
    "        min_block_size=50,\n",
    "        default_reference='GRCh38',\n",
    "        log=HAIL_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = hl.spark_context()\n",
    "config = sc._conf.getAll()\n",
    "config.sort()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load exome capture regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions = hl.import_bed(EXOME_REGIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions.aggregate(hl.agg.counter(ukb_exome_capture_regions.interval.start.contig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_exome_capture_regions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the matrix table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_mt = hl.read_matrix_table(AOU_MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_mt.n_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_mt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to our intervals of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(INTERVALS_TO_EXAMINE) > 0:\n",
    "    aou_mt = hl.filter_intervals(\n",
    "        aou_mt,\n",
    "        [hl.parse_locus_interval(x) for x in INTERVALS_TO_EXAMINE],\n",
    "        keep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to include only exonic variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_mt = aou_mt.filter_rows(hl.is_defined(ukb_exome_capture_regions[aou_mt.locus]), keep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine variants with filter flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_mt_rows = aou_mt.rows()\n",
    "aou_mt_rows.group_by(aou_mt_rows.filters).aggregate(n = hl.agg.count()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Note that AoU VCFs and UKB VCFs use different soft thresholds for what is flagged by VCF filters, so all variant QC happens further downstream. Note that if you do want to make use of the AoU VCF filter flags, uncomment the code in the following two cells.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aou_mt = aou_mt.filter_rows(hl.is_missing(aou_mt.filters), keep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aou_mt_rows = aou_mt.rows()\n",
    "#aou_mt_rows.group_by(aou_mt_rows.filters).aggregate(n = hl.agg.count()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an rsid\n",
    "\n",
    "This is needed by plink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aou_mt = aou_mt.annotate_rows(\n",
    "    rsid = aou_mt.locus.contig + '_' + hl.str(aou_mt.locus.position)\n",
    "            + '_' + aou_mt.alleles[0] + '_' + aou_mt.alleles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert biallelic vaiants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, do not pass the biallelic variants to the split method,\n",
    "# just add the corresponding annotations.\n",
    "aou_bi = aou_mt.filter_rows(hl.len(aou_mt.alleles) == 2)\n",
    "aou_bi = aou_bi.annotate_rows(a_index = 1)\n",
    "aou_bi = aou_bi.annotate_rows(was_split = False)\n",
    "\n",
    "# Split the multi-allelic sites into biallelic sites.\n",
    "aou_multi = aou_mt.filter_rows(hl.len(aou_mt.alleles) > 2)\n",
    "aou_split = hl.split_multi_hts(aou_multi,\n",
    "                               keep_star=False,\n",
    "                               left_aligned=False,\n",
    "                               vep_root='vep',\n",
    "                               permit_shuffle=False)\n",
    "\n",
    "# Union the two collections and include only the row and entry fields that are needed.\n",
    "aou_prepared = aou_split.union_rows(aou_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the matrix table to BGEN\n",
    "\n",
    "https://hail.is/docs/0.2/methods/impex.html#hail.methods.export_bgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homref_gp = hl.literal([1.0, 0.0, 0.0])\n",
    "het_gp = hl.literal([0.0, 1.0, 0.0])\n",
    "homvar_gp = hl.literal([0.0, 0.0, 1.0])\n",
    "\n",
    "aou_prepared = aou_prepared.annotate_entries(\n",
    "    GP = hl.case()\n",
    "        .when(aou_prepared.GT.is_hom_ref(), homref_gp)\n",
    "        .when(aou_prepared.GT.is_het(), het_gp)\n",
    "        .default(homvar_gp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.methods.export_bgen(mt=aou_prepared, output=OUTPUT_BGEN, gp=aou_prepared.GP, rsid=aou_prepared.rsid, parallel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.methods.index_bgen(OUTPUT_BGEN + '.bgen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the Hail log to the workspace bucket so that we can retain it.\n",
    "!gzip --keep {HAIL_LOG}\n",
    "!gsutil cp {HAIL_LOG}.gz {HAIL_LOG_DIR_FOR_PROVENANCE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
